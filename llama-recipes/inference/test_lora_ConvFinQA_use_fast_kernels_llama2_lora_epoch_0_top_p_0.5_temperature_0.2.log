[2023-08-22 03:38:24,461] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.29s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:30<00:16, 16.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:35<00:00, 38.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:35<00:00, 31.83s/it]
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 5120, padding_idx=0)
        (layers): ModuleList(
          (0-39): 40 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): Linear(
                in_features=5120, out_features=5120, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
              (v_proj): Linear(
                in_features=5120, out_features=5120, bias=False
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
              (act_fn): SiLUActivation()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
    )
  )
)
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): PeftModelForCausalLM(
      (base_model): LoraModel(
        (model): LlamaForCausalLM(
          (model): LlamaModel(
            (embed_tokens): Embedding(32001, 5120)
            (layers): ModuleList(
              (0-39): 40 x LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(
                    in_features=5120, out_features=5120, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=5120, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=5120, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
                  (v_proj): Linear(
                    in_features=5120, out_features=5120, bias=False
                    (lora_dropout): ModuleDict(
                      (default): Dropout(p=0.05, inplace=False)
                    )
                    (lora_A): ModuleDict(
                      (default): Linear(in_features=5120, out_features=8, bias=False)
                    )
                    (lora_B): ModuleDict(
                      (default): Linear(in_features=8, out_features=5120, bias=False)
                    )
                    (lora_embedding_A): ParameterDict()
                    (lora_embedding_B): ParameterDict()
                  )
                  (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
                  (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
                  (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
            (norm): LlamaRMSNorm()
          )
          (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
        )
      )
    )
  )
)
  0%|          | 0/5 [00:00<?, ?it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
the inference time is 9804.499881342053 ms

The weighted average exercise price per share in 2007 was $60.94.
 20%|██        | 1/5 [00:09<00:39,  9.87s/it]the inference time is 8497.340133413672 ms

The weighted average exercise price per share in 2005 was $25.14.
 40%|████      | 2/5 [00:18<00:27,  9.10s/it]the inference time is 10740.021890029311 ms

The weighted average exercise price per share in 2007 was $60.94, which is an increase of $35.80 from $25.14 in 2005, representing a 143% change over the years.
 60%|██████    | 3/5 [00:29<00:19,  9.88s/it]the inference time is 8528.030280023813 ms

The weighted average exercise price per share in 2005 was $25.14.
 80%|████████  | 4/5 [00:37<00:09,  9.37s/it]the inference time is 16103.314440697432 ms

The weighted average exercise price per share in 2007 was $60.94, which is an increase of $35.80 from $25.14 in 2005, representing a 143% change over the years. To calculate the change in weighted average exercise price per share from 2005 to 2007, we can use the following formula:

Change in weighted average exercise price per share = (Weighted average exercise price per share in 2007 - Weighted average exercise price per share in 2005) / Weighted average exercise price per share in 2005

100%|██████████| 5/5 [00:53<00:00, 11.82s/it]100%|██████████| 5/5 [00:53<00:00, 10.80s/it]
